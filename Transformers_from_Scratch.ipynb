{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformers from Scratch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmywSNOy5Q60"
      },
      "outputs": [],
      "source": [
        "import torch as th\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, embed_size, heads):\n",
        "      super(SelfAttention, self).__init__()\n",
        "\n",
        "      self.embed_size = embed_size\n",
        "      self.heads = heads\n",
        "      self.head_dim = embed_size // heads # 128\n",
        "\n",
        "      assert (self.head_dim * heads == embed_size), \"Embed size needs to div by heads\"            \n",
        "\n",
        "      self.values = nn.Linear(self.embed_size, self.embed_size, bias = False)\n",
        "      self.keys = nn.Linear(self.embed_size, self.embed_size, bias = False)\n",
        "      self.queries = nn.Linear(self.embed_size, self.embed_size, bias = False)\n",
        "\n",
        "      self.fc_out = nn.Linear(heads * self.head_dim, embed_size) \n",
        "\n",
        "      \n",
        "  def forward(self, values, keys, query, mask): \n",
        "      N = query.shape[0] # query.shape = (batch_size, seq_len, embed_size) -> (N, T, D)\n",
        "      \n",
        "      value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1] \n",
        "\n",
        "      values = self.values(values) # Wv # (N, value_len, D) -> (N, value_len, D) # heads\n",
        "      keys = self.keys(keys) # Wk # (N, key_len, D) -> (N, key_len, D)\n",
        "      query = self.queries(query) # Wq # (N, query_len, D) -> (N, query_len, D)\n",
        "\n",
        "\n",
        "      values = values.reshape(N, value_len, self.heads, self.head_dim) # (N, T, D)-> (N, T, heads, head_dim)\n",
        "      keys = values.reshape(N, key_len, self.heads, self.head_dim)\n",
        "      query = query.reshape(N, query_len, self.heads, self.head_dim)\n",
        "\n",
        "      energy = th.einsum(\"nqhd,nkhd->nhqk\", [query, keys]) # \n",
        "      \n",
        "\n",
        "      if mask is not None:\n",
        "          energy = energy.masked_fill(mask == 0, float(\"-1e20\")) # decodder part masked multhead attention\n",
        "      \n",
        "      attention = th.softmax(energy / (self.embed_size ** 0.5), dim = 3)\n",
        "\n",
        "\n",
        "      out = th.einsum(\"nhql,nlhd->nqhd\", [attention, values])\n",
        "\n",
        "\n",
        "      out = out.reshape(N, query_len, self.embed_size)\n",
        "\n",
        "\n",
        "      out = self.fc_out(out)\n",
        "      return out\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tOY9w5AD5XpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 256\n",
        "batch_size = 64 \n",
        "seq_len = 5\n",
        "heads = 8 # defualt eight is just fine\n",
        "model = SelfAttention(embed_size, heads)\n",
        "x = th.randn((batch_size, seq_len, embed_size))\n",
        "out = model(x, x, x, mask = None)\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Bl6JOprMiis",
        "outputId": "4739810c-6be4-4e27-d67e-9a8f0f2b3c93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 5, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = th.randn((1, 8, 5, 5))\n",
        "score = th.softmax(x, dim = 3)\n",
        "th.sum(score, dim = 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shWCm2CWIuN1",
        "outputId": "e230dfa0-b60b-4663-9f89-d3bd65d23de0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
              "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
              "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
              "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
              "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
              "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
              "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
              "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = SelfAttention(embed_size, heads)\n",
        "        self.layernorm1 = nn.LayerNorm(embed_size)\n",
        "        self.layernorm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            \n",
        "            nn.Linear(embed_size, embed_size * forward_expansion),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embed_size * forward_expansion, embed_size)\n",
        " \n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, value, key, query, mask):\n",
        "        \n",
        "        attention = self.attention(value, key, query, mask)\n",
        "\n",
        "        #print(attention.shape, query.shape)\n",
        "        out = self.dropout(self.layernorm1(attention + query)) # query eklendi fakat burada diger herhangi biri de eklenebilirdi \n",
        "                                                                # fakat eklenemez cunku bu taraf decoder tarafinda da kullaniliyor ve \n",
        "                                                                # cok meshur olan o sekil incelirse query eklenmesi lazim!!\n",
        "\n",
        "\n",
        "        forward = self.feed_forward(self.feed_forward(out))\n",
        "        \n",
        "        return self.dropout(self.layernorm2(forward + out))\n",
        "        \n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "FaJ4swd4OBGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 256\n",
        "heads = 8\n",
        "dropout = 0.4\n",
        "forward_expansion = 4\n",
        "batch_size = 16\n",
        "seq_len = 5\n",
        "model = TransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
        "\n",
        "x = th.randn((batch_size, seq_len, embed_size))\n",
        "score = model(x, x, x, mask = None)\n",
        "score.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9C9Cq2aSNBg",
        "outputId": "0981f7c8-0494-4d75-f3c8-731cb6a16cc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 5, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "\n",
        "  def __init__(self, \n",
        "               src_vocab_size,\n",
        "               embed_size, \n",
        "               num_layers,\n",
        "               heads,\n",
        "               device,\n",
        "               forward_expansion,\n",
        "               dropout,\n",
        "               max_len\n",
        "               ):\n",
        "    \n",
        "      super(Encoder, self).__init__()\n",
        "\n",
        "      self.embed_size = embed_size\n",
        "      self.device = device\n",
        "      self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
        "      self.pos_embedding = nn.Embedding(max_len, embed_size)\n",
        "\n",
        "      self.layers = nn.ModuleList(\n",
        "          \n",
        "          [\n",
        "           TransformerBlock(\n",
        "               embed_size,\n",
        "               heads,\n",
        "               dropout,\n",
        "               forward_expansion\n",
        "           )\n",
        "           for _ in range(num_layers)\n",
        "          ]\n",
        "      )\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "  def forward(self, x, mask):\n",
        "\n",
        "      N, seq_len = x.shape[0], x.shape[1]\n",
        "      pos = th.arange(0, seq_len).expand(N, seq_len).to(self.device)\n",
        "      \n",
        "      out = self.dropout(self.word_embedding(x) + self.pos_embedding(pos))\n",
        "\n",
        "      for layer in self.layers:\n",
        "\n",
        "        out = layer(out, out, out, mask)\n",
        "      return out\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "82PGxSyOJHGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "th.arange(0, 5).expand(4, 5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uegDUPigVoXb",
        "outputId": "72838efa-6e2a-4d38-a3f2-86afb9f39694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 1, 2, 3, 4],\n",
              "        [0, 1, 2, 3, 4],\n",
              "        [0, 1, 2, 3, 4],\n",
              "        [0, 1, 2, 3, 4]])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 256\n",
        "heads = 8\n",
        "dropout = 0.4\n",
        "forward_expansion = 4\n",
        "batch_size = 16\n",
        "seq_len = 5\n",
        "device =  \"cpu\"\n",
        "num_layers = 6\n",
        "src_vocab_size = 10090\n",
        "max_len = 100\n",
        "model = Encoder(src_vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_len)\n",
        "\n",
        "x = th.randint(0, max_len, size = (batch_size, seq_len))\n",
        "score = model(x, mask = None)\n",
        "score.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgYpounHW8xk",
        "outputId": "f032aa81-6aab-47ae-f561-cd6000adb7ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 5, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
        "        super(DecoderBlock,self).__init__()\n",
        "\n",
        "        self.attention = SelfAttention(embed_size, heads)\n",
        "        self.layernorm = nn.LayerNorm(embed_size)\n",
        "        self.transformer_block = TransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, value, key, src_mask, trg_mask): # src_mask padding yapilan ifadelerin islenmemesini sagliyacak!\n",
        "        \n",
        "        attention = self.attention(x, x, x, trg_mask)\n",
        "        query = self.dropout(self.layernorm(attention + x))\n",
        "        out = self.transformer_block(value, key, query, src_mask)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "Oy-LylJDJLHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
        "x = th.randn((batch_size, seq_len, embed_size))\n",
        "value = th.randn((batch_size, seq_len, embed_size))\n",
        "key =  th.randn((batch_size, seq_len, embed_size))\n",
        "\n",
        "score = model(x, value, key, src_mask = None, trg_mask = None)\n",
        "score.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYeyYi8kbbIe",
        "outputId": "3e7ab63b-1a60-4e9c-a6b7-450765a5487e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 5, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, \n",
        "                 trg_vocab_size,\n",
        "                 embed_size,\n",
        "                 num_layers,\n",
        "                 heads,\n",
        "                 forward_expansion,\n",
        "                 dropout,\n",
        "                 device,\n",
        "                 max_len\n",
        "                 ):\n",
        "      \n",
        "        super(Decoder, self).__init__()\n",
        "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
        "        self.pos_embedding = nn.Embedding(max_len, embed_size)\n",
        "        \n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "             DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
        "            for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
        "        self.device = device\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x, encoder_out, src_mask, trg_mask):\n",
        "\n",
        "        N, seq_len = x.shape\n",
        "        pos = th.arange(0, seq_len).expand(N, seq_len).to(self.device)\n",
        "        out = self.dropout(self.word_embedding(x) + self.pos_embedding(pos))\n",
        "\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            out = layer(out, encoder_out, encoder_out, src_mask, trg_mask)\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "3tRppHPgcs3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trg_vocab_size = 10000\n",
        "embed_size = 256\n",
        "num_layers = 6\n",
        "heads = 8\n",
        "forward_expansion = 4\n",
        "dropout = 0.4\n",
        "device = \"cpu\"\n",
        "max_len = 100\n",
        "decoder = Decoder(trg_vocab_size,\n",
        "                 embed_size,\n",
        "                 num_layers,\n",
        "                 heads,\n",
        "                 forward_expansion,\n",
        "                 dropout,\n",
        "                 device,\n",
        "                 max_len\n",
        "                 )\n",
        "x = th.randint(0, trg_vocab_size, size = (batch_size, seq_len))\n",
        "encoder_out = th.randn(batch_size, seq_len, embed_size)\n",
        "score = decoder(x, encoder_out, src_mask = None, trg_mask = None)\n",
        "score.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MD4mOkY6gI-5",
        "outputId": "4785f37d-5a6a-4d57-eeca-33b3cbf548b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 5, 10000])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, \n",
        "                 src_vocab_size,\n",
        "                 trg_vocab_size,\n",
        "                 src_pad_idx,\n",
        "                 trg_pad_idx,\n",
        "                 embed_size = 256,\n",
        "                 num_layer = 6,\n",
        "                 forward_expansion = 4,\n",
        "                 heads = 8,\n",
        "                 dropout = 0,\n",
        "                 device = \"cpu\",\n",
        "                 max_len = 100\n",
        "                 ):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(src_vocab_size,\n",
        "                               embed_size, \n",
        "                               num_layers,\n",
        "                               heads,\n",
        "                               device,\n",
        "                               forward_expansion,\n",
        "                               dropout,\n",
        "                               max_len\n",
        "                               )\n",
        "        self.decoder = Decoder(trg_vocab_size, embed_size, num_layers, heads,\n",
        "                               forward_expansion, dropout, device, \n",
        "                               max_len)\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        src_mask = (src !=self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        return src_mask.to(self.device)\n",
        "\n",
        "    def make_trg_mask(self, trg):\n",
        "        N, trg_len = trg.shape\n",
        "        trg_mask = th.tril(th.ones((trg_len, trg_len)))\n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        encoder_out = self.encoder(src, src_mask)\n",
        "        out = self.decoder(trg, encoder_out, src_mask, trg_mask)\n",
        "        return out"
      ],
      "metadata": {
        "id": "VRsMGDeMhHf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "th.tril(th.ones((5, 5)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLGkxSMJmgRQ",
        "outputId": "69bbf214-7533-44ae-ce96-e0db35823525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0.],\n",
              "        [1., 1., 1., 0., 0.],\n",
              "        [1., 1., 1., 1., 0.],\n",
              "        [1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src = th.randint(0, 10, size = (16, 100))\n",
        "trg = th.randint(0, 10, size = (16, 8))\n",
        "\n",
        "src_pad_idx = 0\n",
        "trg_pad_idx = 0\n",
        "src_vocab_size = 10\n",
        "trg_vocab_size = 10\n",
        "model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx)\n",
        "out = model(src, trg)"
      ],
      "metadata": {
        "id": "4x7fC6Jemjcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npNBuDJanddO",
        "outputId": "8075d874-0d65-4876-8d18-581edf49e15b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 8, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VB3zDJ1an-v6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}